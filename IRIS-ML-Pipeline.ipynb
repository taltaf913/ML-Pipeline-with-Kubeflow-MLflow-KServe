{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Classfier - Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import component, pipeline\n",
    "import kfp\n",
    "from kfp import kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.9\"\n",
    ")\n",
    "def prepare_data(data_path: str):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from sklearn import datasets\n",
    "    \n",
    "    iris = datasets.load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['species'] = iris.target\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df.to_csv(f'{data_path}/final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Split data into Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def train_test_split(data_path: str):    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    final_data = pd.read_csv(f'{data_path}/final_df.csv')\n",
    "    \n",
    "    target_column = 'species'\n",
    "    X = final_data.loc[:, final_data.columns != target_column]\n",
    "    y = final_data.loc[:, final_data.columns == target_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify = y, random_state=47)\n",
    "    \n",
    "    np.save(f'{data_path}/X_train.npy', X_train)\n",
    "    np.save(f'{data_path}/X_test.npy', X_test)\n",
    "    np.save(f'{data_path}/y_train.npy', y_train)\n",
    "    np.save(f'{data_path}/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def training_basic_classifier(data_path: str):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    X_train = np.load(f'{data_path}/X_train.npy',allow_pickle=True)\n",
    "    y_train = np.load(f'{data_path}/y_train.npy',allow_pickle=True)\n",
    "    \n",
    "    classifier = LogisticRegression(max_iter=500)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    import pickle\n",
    "    with open(f'{data_path}/model.pkl', 'wb') as f:\n",
    "        pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model provisioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def register_model(data_path: str, aws_access_key_id: str, aws_secret_access_key: str, aws_default_region: str) -> dict:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    import mlflow\n",
    "    from mlflow.models import infer_signature\n",
    "    from sklearn import datasets\n",
    "   \n",
    "    with open(f'{data_path}/model.pkl','rb') as f:\n",
    "        logistic_reg_model = pickle.load(f)\n",
    "    \n",
    "    # Infer the model signature\n",
    "    X_test = np.load(f'{data_path}/X_test.npy', allow_pickle=True)\n",
    "    y_pred = logistic_reg_model.predict(X_test)\n",
    "    signature = infer_signature(X_test, y_pred)\n",
    "    \n",
    "    # Set AWS credentials in the environment\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = aws_default_region\n",
    "    \n",
    "    # log and register the model using MLflow scikit-learn API\n",
    "    mlflow.set_tracking_uri(\"http://mlflowserver.kubeflow:5000\")\n",
    "    reg_model_name = \"SklearnLogisticRegression\"\n",
    "    \n",
    "    experiment_id = mlflow.create_experiment(\"test-1\") \n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "        mlflow.log_param('max_iter', 500)\n",
    "\n",
    "        # Log model artifact to S3\n",
    "        artifact_path = \"sklearn-model\"      \n",
    "        mlflow.log_artifact(local_path=f'{data_path}/model.pkl', artifact_path=artifact_path)\n",
    "        \n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=logistic_reg_model,\n",
    "            artifact_path=\"sklearn-model\",\n",
    "            signature=signature,\n",
    "            registered_model_name=reg_model_name,\n",
    "        )\n",
    "    \n",
    "    model_uri = f\"runs:/{run.info.run_id}/sklearn-model\"\n",
    "    \n",
    "    # Register model linked to S3 artifact location          \n",
    "    mlflow.register_model(\n",
    "        model_uri,\n",
    "        reg_model_name\n",
    "    )\n",
    "\n",
    "    return {\"artifact_path\": artifact_path, \"artifact_uri\": run.info.artifact_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"mlflow\", \"boto3\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def predict_on_test_data(data_path: str, model_info: dict, aws_access_key_id: str, aws_secret_access_key: str, aws_default_region: str) -> str:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    import mlflow\n",
    "    \n",
    "    # Set AWS credentials in the environment\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = aws_default_region\n",
    "    \n",
    "    artifact_path = model_info[\"artifact_path\"]\n",
    "    artifact_uri = model_info[\"artifact_uri\"]\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://mlflowserver.kubeflow:5000\")\n",
    "    model_uri = f\"{artifact_uri}/{artifact_path}\"\n",
    "    logistic_reg_model = mlflow.sklearn.load_model(model_uri)\n",
    "        \n",
    "    X_test = np.load(f'{data_path}/X_test.npy',allow_pickle=True)\n",
    "    y_pred = logistic_reg_model.predict(X_test)\n",
    "    np.save(f'{data_path}/y_pred.npy', y_pred)\n",
    "    \n",
    "    X_test = np.load(f'{data_path}/X_test.npy',allow_pickle=True)\n",
    "    y_pred_prob = logistic_reg_model.predict_proba(X_test)\n",
    "    np.save(f'{data_path}/y_pred_prob.npy', y_pred_prob)\n",
    "    \n",
    "    return model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"kserve\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def model_serving(model_uri: str):\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    import os\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    \n",
    "    name='sklearn-iris-v2'\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                   service_account_name='mlflow-sa',\n",
    "                                   sklearn=(V1beta1SKLearnSpec(storage_uri=model_uri)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. The complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client, config\n",
    "import base64\n",
    "\n",
    "@pipeline(\n",
    "    name=\"iris-pipeline\",\n",
    ")\n",
    "def iris_pipeline(data_path: str):\n",
    "    pvc1 = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name_suffix='-iris-mlflow-pvc',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1Mi',\n",
    "        storage_class_name='standard'\n",
    "    )\n",
    "    \n",
    "    # Load Kubernetes configuration\n",
    "    config.load_kube_config()\n",
    "\n",
    "    # Fetch the AWS credentials from the secret\n",
    "    secret_name = \"aws-credentials\"\n",
    "    secret_namespace = \"kubeflow\"\n",
    "    secret_key_id = \"AWS_ACCESS_KEY_ID\"\n",
    "    secret_key_access = \"AWS_SECRET_ACCESS_KEY\"\n",
    "    secret_region = \"AWS_DEFAULT_REGION\"\n",
    "\n",
    "    v1 = client.CoreV1Api()\n",
    "    secret = v1.read_namespaced_secret(secret_name, namespace=secret_namespace)\n",
    "    \n",
    "    # Convert bytes to string\n",
    "    aws_access_key_id = base64.b64decode(secret.data[secret_key_id]).decode('utf-8')\n",
    "    aws_secret_access_key = base64.b64decode(secret.data[secret_key_access]).decode('utf-8')\n",
    "    aws_default_region = base64.b64decode(secret.data[secret_region]).decode('utf-8')\n",
    "    \n",
    "    prepare_data_task = prepare_data(data_path=data_path)\n",
    "    kubernetes.mount_pvc(prepare_data_task, pvc_name=pvc1.outputs['name'], mount_path='/data')\n",
    "    \n",
    "    train_test_split_task = train_test_split(data_path=data_path) \n",
    "    kubernetes.mount_pvc(train_test_split_task, pvc_name=pvc1.outputs['name'], mount_path='/data')\n",
    "    train_test_split_task.after(prepare_data_task)\n",
    "    \n",
    "    training_basic_classifier_task = training_basic_classifier(data_path=data_path)\n",
    "    kubernetes.mount_pvc(training_basic_classifier_task, pvc_name=pvc1.outputs['name'], mount_path='/data')\n",
    "    training_basic_classifier_task.after(train_test_split_task)\n",
    "    \n",
    "    register_model_task = register_model(data_path=data_path, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_default_region=aws_default_region)\n",
    "    kubernetes.mount_pvc(register_model_task, pvc_name=pvc1.outputs['name'], mount_path='/data')\n",
    "    kubernetes.mount_pvc(register_model_task, pvc_name=\"mlflow-pvc\", mount_path='/opt/mlflow/')\n",
    "    register_model_task.after(training_basic_classifier_task)\n",
    "    \n",
    "    predict_on_test_data_task = predict_on_test_data(data_path=data_path, model_info=register_model_task.output, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, aws_default_region=aws_default_region)\n",
    "    kubernetes.mount_pvc(predict_on_test_data_task, pvc_name=pvc1.outputs['name'], mount_path='/data')\n",
    "    predict_on_test_data_task.after(register_model_task)\n",
    "    \n",
    "    model_serving_task = model_serving(model_uri=predict_on_test_data_task.output)\n",
    "    model_serving_task.after(predict_on_test_data_task)\n",
    "    \n",
    "    delete_pvc1 = kubernetes.DeletePVC(pvc_name=pvc1.outputs['name']).after(model_serving_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Initiate ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=iris_pipeline,\n",
    "    package_path='iris_mlflow_kserve_pipeline.yaml'\n",
    ")\n",
    "\n",
    "client = kfp.Client(host='http://localhost:8080')\n",
    "client.create_run_from_pipeline_func(\n",
    "    iris_pipeline, arguments={\n",
    "        'data_path': '/data'\n",
    "    }, enable_caching=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
